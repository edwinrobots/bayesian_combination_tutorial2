{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increasing-blind",
   "metadata": {},
   "source": [
    "# Aggregating and Learning from Multiple Annotators: Practical\n",
    "\n",
    "This part of the tutorial has the following goals:\n",
    "   * Learn to use Dawid and Skene (1979) models to aggregate classifications from a crowd.\n",
    "   * Understand the effect of key hyperparameters in the above models.\n",
    "   \n",
    "As an example dataset, we will work with crowdsourced annotations for the _recognising textual entailment (RTE)_ task. This dataset is the PASCAL RTE-1 corpus containing 800 text-hypothesis pairs for which gold annotations were provided by Dagan et al. (2006). Snow et al. (2008) produced the crowdsourced dataset for the corpus:  164 annotators produced 10 annotations for each sentence-pair. \n",
    "\n",
    "> Ido  Dagan,  Oren  Glickman,  and  Bernardo  Magnini.2006.  The PASCAL recognising textual entailmentchallenge.In  Juan  Qui ̃nonero-Candela,  Ido  Da-gan, Bernardo Magnini, and F. d’Alch ́e Buc, editors,MLCW 2005, number 3944 in LNAI, pages 177–190.Springer.\n",
    "\n",
    "> Rion Snow,  Brendan O’Connor,  Daniel Jurafsky,  andAndrew  Y.  Ng.  2008.    Cheap  and  fast  -  but  is  itgood? evaluating non-expert annotations for naturallanguage tasks. InProc. of EMNLP, pages 254–263.\n",
    "\n",
    "First, we apply a variational Bayesian variant of Dawid & Skene's 1979 method to infer the true labels independently. Then, we train classifiers with both aggregated labels and the raw crowdsourced labels using the CrowdLayer method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-visit",
   "metadata": {},
   "source": [
    "## Importing Required Packages\n",
    "Please run the notebook containing the directory you cloned from the Github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up any changes to the imported modules automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from scipy.stats import beta\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "# code included in this directory:\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "from bayesian_combination.ibcc import IBCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-skating",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "The following code should show that the data folders 'snow2008_mturk_data_with_orig_files_assembled_201904' and 'all_collected_data' are in the current directory and the 'rte' folder is inside 'snow2008_mturk_data_with_orig_files_assembled_201904'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('.'))\n",
    "print(os.listdir('./snow2008_mturk_data_with_orig_files_assembled_201904'))\n",
    "\n",
    "rte_text_dir = './snow2008_mturk_data_with_orig_files_assembled_201904/rte'\n",
    "rte_text_file = os.path.join(rte_text_dir, 'rte1.tsv')\n",
    "rte_anno_file = './all_collected_data/rte.standardized.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-thailand",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the text.\n",
    "textdata = pd.read_csv(\n",
    "    rte_text_file, \n",
    "    sep='\\t', \n",
    "    usecols=[0, 1, 3, 4], \n",
    "    index_col=0, \n",
    "    names=['item_id', 'gold', 'text', 'hypothesis'],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "# Load the crowdsourced data\n",
    "crowddata = pd.read_csv(\n",
    "    rte_anno_file, \n",
    "    sep='\\t', \n",
    "    usecols=[0, 1, 2, 3],\n",
    "    index_col=0,\n",
    "    names=['annotation_id', 'worker', 'item_id', 'label'],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "print('Gold-labelled text data: \\n')\n",
    "\n",
    "for column in textdata:\n",
    "    print(f'>{column}: {textdata[column].iloc[0]}')\n",
    "\n",
    "print('\\n\\nCrowdsourced labels: \\n')\n",
    "    \n",
    "for column in crowddata:\n",
    "    print(f'>{column}: {crowddata[column].iloc[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the gold data into a vector\n",
    "num_items = np.max(textdata.index) + 1\n",
    "\n",
    "gold_labels = np.zeros(num_items) - 1\n",
    "gold_labels[textdata.index] = textdata[\"gold\"].astype(int)\n",
    "\n",
    "print(np.sum(gold_labels==0))\n",
    "print(np.sum(gold_labels==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Amazon MT worker IDs to consecutive numbers\n",
    "uworkers, worker_ids = np.unique(crowddata[\"worker\"], return_inverse=True)\n",
    "num_workers = np.max(worker_ids) + 1\n",
    "\n",
    "# Put the crowdsourced data into matrix format\n",
    "crowd_matrix = np.zeros((num_items, num_workers)) - 1  # use -1 to indicate missing labels\n",
    "\n",
    "# fill the matrix\n",
    "crowd_matrix[crowddata[\"item_id\"].astype(int), worker_ids] = crowddata[\"label\"].astype(int)\n",
    "\n",
    "print(crowd_matrix)\n",
    "print(np.sum(crowd_matrix == 0))\n",
    "print(np.sum(crowd_matrix == 1))\n",
    "\n",
    "# limit the number of labels per data point to 5\n",
    "def reduce_num_annotations(crowd_matrix, max_workers_per_item):\n",
    "    for i in range(num_items):\n",
    "        row = crowd_matrix[i, :]\n",
    "        available = np.argwhere(row > -1).flatten()\n",
    "        if len(available) > max_workers_per_item:\n",
    "            to_be_removed = available[max_workers_per_item:]\n",
    "            crowd_matrix[i, to_be_removed] = -1\n",
    "            \n",
    "    return crowd_matrix\n",
    "\n",
    "max_workers_per_item = 3\n",
    "crowd_matrix = reduce_num_annotations(crowd_matrix, max_workers_per_item)\n",
    "\n",
    "print(np.unique(np.sum(crowd_matrix > -1, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-cinema",
   "metadata": {},
   "source": [
    "Let's test some baselines. Firstly, the majority class baseline, then the majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions = np.zeros(num_items)\n",
    "\n",
    "total_votes = np.sum(crowd_matrix != -1, axis=1)\n",
    "votes = np.zeros(num_items)\n",
    "votes[total_votes > 0] = np.sum(crowd_matrix == 1, axis=1)[total_votes > 0] / total_votes[total_votes > 0].astype(float)\n",
    "\n",
    "mv_predictions = np.zeros(num_items, dtype=int)\n",
    "mv_predictions[(total_votes > 0) & (votes>0.5)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-sterling",
   "metadata": {},
   "source": [
    "Write a function to compute and show some performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(gold_labels, prob_class_1, predicted_labels, method_names):\n",
    "        \n",
    "    idxs = gold_labels != -1\n",
    "    gold_labels = gold_labels[idxs]\n",
    "        \n",
    "    for m, method_name in enumerate(method_names):\n",
    "        print('Method = {}'.format(method_name))\n",
    "        \n",
    "        acc = accuracy_score(gold_labels, predicted_labels[m][idxs])\n",
    "        print('Accuracy = {}'.format(acc))\n",
    "    \n",
    "        f1 = f1_score(gold_labels, predicted_labels[m][idxs])\n",
    "        print('F1 score = {}'.format(f1))\n",
    "        \n",
    "        ll = log_loss(gold_labels, prob_class_1[m][idxs])\n",
    "        print('Cross entropy error = {}'.format(ll))\n",
    "        \n",
    "show_metrics(\n",
    "    gold_labels, \n",
    "    [baseline_predictions, votes], \n",
    "    [baseline_predictions, mv_predictions], \n",
    "    ['majority class', 'majority_vote']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-steam",
   "metadata": {},
   "source": [
    "## Dawid & Skene (IBCC-VB)\n",
    "\n",
    "The model we run here is IBCC-VB, which is a modification to the Dawid & Skene (1979) model. The [repository is available here](https://github.com/ukplab/arxiv2018-bayesian-ensembles). For a recap of the model, please [see the tutorial slides](https://sites.google.com/view/alma-tutorial) or the description in [Paun et al. (2018), Comparing Bayesian Models of Annotation](https://transacl.org/ojs/index.php/tacl/article/view/1430).\n",
    "\n",
    "\n",
    "\n",
    "IBCC-VB differs from the original Dawid & Skene method in the following ways:\n",
    "   * The parameters have prior distributions -- it is a Bayesian treatment of the model.\n",
    "   * Approximate inference is performed using variational Bayes instead of maximum likelihood EM.\n",
    "This has some benefits: \n",
    "   * We can include prior information about labeller reliability and class imbalances. This is particularly useful when to avoid the nonidentifiability problem of unsupervised methods by encoding the prior belief that labellers are more likely to give correct answers than incorrect ones.\n",
    "   * The inference method integrates over unknown parameters, reducing the influence of workers who are known with less confidence. For example, if a worker labels two data points and gets them right, how strongly do we trust them on a third data point? Usually, we would not be very conifdent of their reliability despite a 100% accuracy record.  We may even trust them less than someone who labelled 1000 data points and got 900 right. Bayesian inference takes care of this, whereas maximum likelihood estimation can be led astray by workers who have completed few tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-testament",
   "metadata": {},
   "source": [
    "### Run IBCC with a Matrix of Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibcc_model = IBCC(\n",
    "    L=2, # number of classes\n",
    "    K=num_workers, \n",
    ")\n",
    "\n",
    "# Run inference to obtain an array of size num_data_points x num_classes containing the probabilities of the true labels\n",
    "posterior, ibcc_predictions, _ = ibcc_model.fit_predict(crowd_matrix)\n",
    "\n",
    "# Compare with the ground truth\n",
    "show_metrics(\n",
    "    gold_labels, \n",
    "    [baseline_predictions, votes, posterior[:, 1]], \n",
    "    [baseline_predictions, mv_predictions, ibcc_predictions], \n",
    "    ['majority class', 'majority_vote', 'IBCC-VB']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-canberra",
   "metadata": {},
   "source": [
    "## Evaluating Annotators\n",
    "\n",
    "The IBCC-VB model learns a 'confusion matrix' parameter for each annotator. This parameter describes the reliability of the worker. IBCC-VB provides the expected value of this parameter, as shown in the code below. Each row of the matrix is the likelihood distribution of the annotator's labels given the ground truth. High values on the diagonals indicate an accurate worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix for worker i\n",
    "def plot_confusion_matrix(model, annotator_idx):\n",
    "    \n",
    "    # compute the expected confusion matrix by normalising the alpha variational parameter\n",
    "    E_conf_mat = model.A.alpha[:, :, annotator_idx] / np.sum(model.A.alpha[:, :, annotator_idx], axis=1)[:, None]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(E_conf_mat)\n",
    "    \n",
    "    min_val, max_val = 0, model.L\n",
    "    ind_array_rows = np.arange(min_val, max_val, 1)\n",
    "    \n",
    "    min_val, max_val = 0, model.L\n",
    "    ind_array_cols = np.arange(min_val, max_val, 1)\n",
    "    \n",
    "    x, y = np.meshgrid(ind_array_rows, ind_array_cols)\n",
    "\n",
    "    for i, (x_val, y_val) in enumerate(zip(x.flatten(), y.flatten())):\n",
    "        text = \"%.2f\" % E_conf_mat[y_val, x_val]\n",
    "        txt = plt.text(x_val, y_val, text, va='center', ha='center')\n",
    "        txt.set_path_effects([PathEffects.withStroke(linewidth=5, foreground='w')])\n",
    "        \n",
    "    plt.xlabel('annotator label')\n",
    "    plt.ylabel('true label')\n",
    "    plt.yticks([0, 1])\n",
    "    plt.xticks([0, 1])\n",
    "\n",
    "        \n",
    "# Show the results with a selection of annotators:\n",
    "plot_confusion_matrix(ibcc_model, 0)\n",
    "plot_confusion_matrix(ibcc_model, 1)\n",
    "plot_confusion_matrix(ibcc_model, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-saturday",
   "metadata": {},
   "source": [
    "If we need a single value to assess an annotator's competence, we can compute the mutual information (MI) between the annotators' labels and the ground truth. This value uses information theory to quantify how much we learn about the true label from an annotator's label. In other words the information gain (IG) from the annotation.\n",
    "\n",
    "Assuming an unlabelled data point, the information gain about a label $c_i$ from annotator $n$'s label, $y_{i,n}$ is:\n",
    "\n",
    "$$\n",
    "I(c_i; y_{i,n}) = H(c_{i}) - H(c_i | y_{i,n}) \\\\\n",
    "= \\sum_{j=1}^J\\left\\{ - p(c_i=j) \\ln p(c_i=j) + \\sum_{k=1}^J p(y_{i,n}=k)p(c_i=j | y_{i,n}=k) \\ln p(c_i=j | y_{i,n}=k) \\right\\} \\\\\n",
    "= \\sum_{j=1}^J \\left\\{ - p(c_i=j) \\ln p(c_i=j) + \\sum_{k=1}^J p(y_{i,n}=k | c_i=j)p(c_i=j) \\ln \\frac{p(y_{i,n}=k | c_i=j)p(c_i=j)}{\\sum_{l=1}^J p(y_{i,n}=l | c_i=j)p(c_i=j)} \\right\\}\n",
    "$$\n",
    "\n",
    "where $J$ is the number of classes and $H$ is entropy.\n",
    "\n",
    "This function is already implemented in the BayesianCombination library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = ibcc_model.informativeness()  # get a list of informativeness scores for all annotators\n",
    "\n",
    "ranks = np.flip(np.argsort(I))  # sort in descending order\n",
    "\n",
    "# Print the top annotators' scores and confusion matrices\n",
    "for i in range(5):\n",
    "    print(\"Annotator {} has informativeness score of {}\".format(ranks[i], I[ranks[i]]))\n",
    "#     plot_confusion_matrix(ibcc_model, ranks[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-lewis",
   "metadata": {},
   "source": [
    "### Adjusting the hyperparameters\n",
    "\n",
    "In IBCC, each worker has a confusion matrix parameter for each worker that determines the likelihood of the worker labels given the true classes. Each row of this matrix, $\\boldsymbol\\pi_j$, contains the probabilities of a categorical distribution for the case when the true label has value $j$. These probabilities have a Dirichlet prior distribution, $\\boldsymbol\\pi_j \\sim Dirichlet(\\boldsymbol\\alpha_{0,j})$. The hyperparameter vectors $\\boldsymbol\\alpha_{0,j}$ are rows of the hyperparameter matrix, $\\boldsymbol\\alpha$.\n",
    "\n",
    "By default, the IBCC code sets the values in $\\boldsymbol\\alpha$ to $1$ on the off-diagonals and 2 on the diagonals. Larger values on the diagonals increase the probability that workers tend to give the right answer, because the diagonals correspond to giving correct answers.\n",
    "\n",
    "However, we can set $\\boldsymbol\\alpha$ to any suitable values for a particular domain. \n",
    "\n",
    "The model also has a hyperparameter $\\boldsymbol\\beta_0$ that also parametrises a Dirichlet distribution over the class prior. Adjusting this encodes the prior belief in the balance between the two classes.\n",
    "\n",
    "To understand the effects of $\\boldsymbol\\alpha_0$ and $\\boldsymbol\\beta_0$, let's first visualise the Dirichlet distribution to see how it maps $\\boldsymbol\\alpha_{0,j}$ to a distribution over probabilities. In our use case, we have only two classes, so we can work with the binary special case of the Dirichlet known as the Beta distribution. For the binary case, we compute the probability density function over p(class 1) only, since p(class 0) = 1 - p(class 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's plot some beta distriutions for different parameters:\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Uncertainty over whether a worker is accurate or not')\n",
    "\n",
    "x = np.arange(101) / 100.0\n",
    "y = beta.pdf(x, 1, 1)\n",
    "plt.plot(x,y, label='1, 1: noninformative prior -- no prior knowledge')\n",
    "\n",
    "y = beta.pdf(x, 0.1, 0.1)\n",
    "plt.plot(x,y, label='0.1, 0.1: extreme, but uncertain which extreme')\n",
    "\n",
    "y = beta.pdf(x, 0.8, 0.8)\n",
    "plt.plot(x,y, label='0.8, 0.8: bias to the extremes')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Weak priori belief that the worker is accurate')\n",
    "\n",
    "y = beta.pdf(x, 1, 0.1)\n",
    "plt.plot(x,y, label='1, 1.1')\n",
    "\n",
    "y = beta.pdf(x, 2, 1)\n",
    "plt.plot(x,y, label='1, 2')\n",
    "\n",
    "y = beta.pdf(x, 10, 9)\n",
    "plt.plot(x,y, label=', 5')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Strong priori belief that the worker is accurate')\n",
    "\n",
    "y = beta.pdf(x, 1, 0.1)\n",
    "plt.plot(x,y, label='1, 0.1')\n",
    "\n",
    "y = beta.pdf(x, 10, 1)\n",
    "plt.plot(x,y, label='10, 1')\n",
    "\n",
    "y = beta.pdf(x, 10, 5)\n",
    "plt.plot(x,y, label='10, 5')\n",
    "\n",
    "y = beta.pdf(x, 100, 50)\n",
    "plt.plot(x,y, label='100, 50')\n",
    "\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-composer",
   "metadata": {},
   "source": [
    "Now, we can plot performance (prec, rec, f1) as we vary $\\boldsymbol\\alpha_0$ then $\\boldsymbol\\beta_0$. To reduce the number of hyperparameters we have to vary, we are going to consider just two values: a value that is first assigned to all entries, and a value that is added to all diagonals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def run_ibcc(diagonals, base, beta0, gold_labels, crowd_matrix):\n",
    "\n",
    "    ibcc_model = IBCC(\n",
    "        L=2, # number of classes\n",
    "        K=num_workers, \n",
    "        alpha0_factor=base,\n",
    "        alpha0_diags=diagonals,\n",
    "        beta0_factor=beta0\n",
    "    )\n",
    "\n",
    "    # Run inference to obtain an array of size num_data_points x num_classes containing the probabilities of the true labels\n",
    "    posterior, predictions, _ = ibcc_model.fit_predict(crowd_matrix)\n",
    "    \n",
    "    idxs = gold_labels != -1\n",
    "    acc = accuracy_score(gold_labels[idxs], predictions[idxs])\n",
    "    f1 = f1_score(gold_labels[idxs], predictions[idxs])\n",
    "    \n",
    "    return acc, f1\n",
    "\n",
    "range_diagonals = np.array([0.01, 0.1, 1, 10, 100])\n",
    "range_base = np.array([0.01, 0.1, 1, 10, 100])\n",
    "\n",
    "results_acc = np.zeros((len(range_base), len(range_diagonals)))\n",
    "results_f1 = np.zeros((len(range_base), len(range_diagonals)))\n",
    "\n",
    "for i, o in enumerate(range_base):\n",
    "    for j, d in enumerate(range_diagonals):\n",
    "        acc, f1 = run_ibcc(d, o, 1, gold_labels, crowd_matrix)\n",
    "        \n",
    "        results_acc[i, j] = acc\n",
    "        results_f1[i, j] = f1\n",
    "\n",
    "# bestbase, bestdiag = np.unravel_index(np.argmax(results_acc), (len(range_base), len(range_diagonals)))\n",
    "# print('Best results: base value={}, diagonal={}, accuracy={}'.format(\n",
    "#     range_base[bestbase],\n",
    "#     range_diagonals[bestdiag],\n",
    "#     results_acc[bestbase, bestdiag]\n",
    "# ) )\n",
    "        \n",
    "bestbase, bestdiag = np.unravel_index(np.argmax(results_f1), (len(range_base), len(range_diagonals)))\n",
    "print('Best results: base value={}, diagonal={}, F1 score={}'.format(\n",
    "    range_base[bestbase],\n",
    "    range_diagonals[bestdiag],\n",
    "    results_f1[bestbase, bestdiag]\n",
    ") )\n",
    "  \n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.plot_surface(\n",
    "#     np.tile(np.log10(range_base)[None, :], (len(range_diagonals), 1)), \n",
    "#     np.tile(np.log10(range_diagonals)[:, None], (1, len(range_base))), \n",
    "#     results_acc,\n",
    "#     cmap=cm.coolwarm,\n",
    "#     linewidth=0, \n",
    "#     antialiased=False\n",
    "# )\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(\n",
    "    np.tile(np.log10(range_base)[None, :], (len(range_diagonals), 1)), \n",
    "    np.tile(np.log10(range_diagonals)[:, None], (1, len(range_base))), \n",
    "    results_f1,\n",
    "    cmap=cm.coolwarm,\n",
    "    linewidth=0, \n",
    "    antialiased=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it again, this time varying beta0 and keeping alpha0 fixed\n",
    "range_proportions = np.array([0.01, 0.1, 1, 10, 100])\n",
    "\n",
    "results_acc = np.zeros(len(range_proportions))\n",
    "results_f1 = np.zeros(len(range_proportions))\n",
    "\n",
    "for i, p in enumerate(range_proportions):\n",
    "    acc, f1 = run_ibcc(1, 1, p, gold_labels, crowd_matrix)\n",
    "\n",
    "    results_acc[i] = acc\n",
    "    results_f1[i] = f1\n",
    "\n",
    "bestbeta = np.argmax(results_acc)\n",
    "print('Best results: beta0={}, accuracy={}'.format(\n",
    "    range_proportions[bestbeta],\n",
    "    results_acc[bestbeta]\n",
    ") )\n",
    "        \n",
    "bestbeta = np.argmax(results_f1)\n",
    "print('Best results: beta0={}, F1 score={}'.format(\n",
    "    range_proportions[bestbeta],\n",
    "    results_f1[bestbeta]\n",
    ") )\n",
    "  \n",
    "fig = plt.figure()\n",
    "plt.plot(\n",
    "    np.log10(range_proportions),\n",
    "    results_f1,\n",
    "    label='f1'\n",
    ")\n",
    "plt.plot(\n",
    "    np.log10(range_proportions),\n",
    "    results_acc,\n",
    "    label='accuracy'\n",
    ")\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144870e-acfe-4748-8e99-060f7b4138f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
